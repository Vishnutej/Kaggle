{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of loss Variable is 3036.19545173\n",
      "Median of loss Variable is 2113.78\n",
      "Standard Deviation of loss Variable is 2930.01187561\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "train_url = \"C:\\Users\\Vishnutej\\Desktop\\pml_train.csv\"\n",
    "train_set = pd.read_csv(train_url)\n",
    "#print(train.head())\n",
    "test_url = \"C:\\Users\\Vishnutej\\Desktop\\pml_test_features.csv\"\n",
    "test_set =pd.read_csv(test_url)\n",
    "#print(train[\"id\"].value_counts())\n",
    "train,test = train_test_split(train,test_size=0.1,random_state=42)\n",
    "#All the features excluding the labels\n",
    "features = [x for x in train.columns if x not in ['id','loss']]\n",
    "#print(features)\n",
    "\n",
    "#Categorical Features\n",
    "cat_features = [x for x in train.select_dtypes(include=['object']).columns if x not in ['id','loss']]\n",
    "#print(cat_features)\n",
    "\n",
    "#Numerical Features\n",
    "num_features = [x for x in train.select_dtypes(exclude=['object']).columns if x not in ['id','loss']]\n",
    "#print(num_features)\n",
    "\n",
    "\n",
    "print ('Mean of loss Variable'+' '+'is'+' '+ str(np.mean(train[\"loss\"])))\n",
    "print ('Median of loss Variable'+' '+'is'+' '+ str(np.median(train[\"loss\"])))\n",
    "print ('Standard Deviation of loss Variable'+' '+'is'+' '+ str(np.std(train[\"loss\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Loss \n",
    "\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "fig=sm.qqplot(train[\"loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting Log(loss)\n",
    "\n",
    "from scipy.stats import norm, lognorm\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train['log_loss'] = np.log(train['loss'])\n",
    "\n",
    "# Fit the log(loss)\n",
    "(mu, sigma) = norm.fit(train['log_loss'])\n",
    "\n",
    "# Histogram\n",
    "n, bins, patches = plt.hist(train['log_loss'], 60, normed=1, facecolor='black', alpha=0.5)\n",
    "\n",
    "y = mlab.normpdf( bins, mu, sigma)\n",
    "l = plt.plot(bins, y, 'r--', linewidth=2)\n",
    "\n",
    "#Graph\n",
    "plt.xlabel('Ln(loss)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title(r'$\\mathrm{Histogram\\ of\\ Ln(Loss):}\\ \\mu=%.3f,\\ \\sigma=%.3f$' %(mu, sigma))\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert categorical variables\n",
    "\n",
    "for each in cat_features:\n",
    "    train[each]=pd.factorize(train[each], sort=True)[0]\n",
    "    test[each]=pd.factorize(test[each],sort=True)[0]\n",
    "    test_set[each]=pd.factorize(test_set[each],sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForestRegressor is 1277.86652447\n",
      "Accuracy of RandomForestRegressor is 1227.36306005\n"
     ]
    }
   ],
   "source": [
    "#Random forest regressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "Predictors= train.ix[:,0:130]\n",
    "Predictors_test= test.ix[:,0:130]\n",
    "\n",
    "MAE=[]\n",
    "Model_Name=[]\n",
    "#rf = RandomForestRegressor(n_estimator=100) \n",
    "rfs = [RandomForestRegressor(),RandomForestRegressor(n_estimators=100)]\n",
    "for rf in rfs: \n",
    "    Model=rf.fit(Predictors,train['log_loss'])\n",
    "    Prediction= np.exp(Model.predict(Predictors_test))\n",
    "    eva = mae(test['loss'],Prediction)\n",
    "    #print(str(eva))\n",
    "    MAE.append(eva)\n",
    "    Name=rf.__class__.__name__\n",
    "    Model_Name.append(Name)\n",
    "    print('Accuracy of'+ ' '+Name+' '+'is'+' '+str(eva))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Lasso is 1810.71939472\n",
      "Accuracy of Lasso is 1492.80461683\n",
      "[1277.8665244738083, 1227.363060045765, 1810.7193947150668, 1492.8046168294381]\n"
     ]
    }
   ],
   "source": [
    "#Lasso\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#las = [Lasso(alpha=0.1),Lasso(alpha=1.0)]\n",
    "lass=[Lasso(alpha=1.0),Lasso(alpha=0.1)]\n",
    "\n",
    "for las in lass:\n",
    "    Model=las.fit(Predictors,train['log_loss'])\n",
    "    Prediction= np.exp(Model.predict(Predictors_test))\n",
    "    eva = mae(test['loss'],Prediction)\n",
    "    MAE.append(eva)\n",
    "    Name=las.__class__.__name__\n",
    "    Model_Name.append(Name)\n",
    "    print('Accuracy of'+ ' '+Name+' '+'is'+' '+str(eva))\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison of the models.\n",
    "Index = [1,2,3,4]\n",
    "plt.bar(Index,MAE)\n",
    "plt.xticks(Index, Model_Name,rotation=45)\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Model')\n",
    "plt.title('MAE of Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named xgboost",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-48c8d1f0cff2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtraining_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPredictors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtesting_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPredictors_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'log_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named xgboost"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "           id         loss\n",
      "0      131822  3933.117822\n",
      "1      131823  3689.457017\n",
      "2      131824  5144.664605\n",
      "3      131825  3618.703123\n",
      "4      131826  4605.443110\n",
      "5      131827  3892.943937\n",
      "6      131828  3835.883803\n",
      "7      131829  3629.304423\n",
      "8      131830  8943.294749\n",
      "9      131831  3995.404012\n",
      "10     131832  4114.210877\n",
      "11     131833  4156.898973\n",
      "12     131834  3604.965323\n",
      "13     131835  5081.743458\n",
      "14     131836  3994.577412\n",
      "15     131837  2438.796133\n",
      "16     131838  4290.043713\n",
      "17     131839  3686.471823\n",
      "18     131840  4255.396426\n",
      "19     131841  4486.837602\n",
      "20     131842  2419.824662\n",
      "21     131843  2437.295763\n",
      "22     131844  4937.930294\n",
      "23     131845  2970.094912\n",
      "24     131846  3583.217210\n",
      "25     131847  3253.377430\n",
      "26     131848  2671.008927\n",
      "27     131849  5211.988534\n",
      "28     131850  4283.896597\n",
      "29     131851  4365.855717\n",
      "...       ...          ...\n",
      "56466  188288  1711.397125\n",
      "56467  188289  2448.650256\n",
      "56468  188290  3885.714837\n",
      "56469  188291  4043.219208\n",
      "56470  188292  3710.332730\n",
      "56471  188293  4065.459806\n",
      "56472  188294  2268.019512\n",
      "56473  188295  3875.310891\n",
      "56474  188296  4200.779076\n",
      "56475  188297  4359.444134\n",
      "56476  188298  5137.339679\n",
      "56477  188299  3358.337204\n",
      "56478  188300  1785.308600\n",
      "56479  188301  3912.903339\n",
      "56480  188302  7286.605519\n",
      "56481  188303  4539.238570\n",
      "56482  188304  3868.976557\n",
      "56483  188305  3882.135589\n",
      "56484  188306  3924.377819\n",
      "56485  188307  4264.706426\n",
      "56486  188308  3893.149229\n",
      "56487  188309  3648.407869\n",
      "56488  188310  3478.435323\n",
      "56489  188311  4284.414993\n",
      "56490  188312  3899.200392\n",
      "56491  188313  3964.770394\n",
      "56492  188314  3491.350662\n",
      "56493  188315  4397.438320\n",
      "56494  188316  4343.018560\n",
      "56495  188317  3380.744885\n",
      "\n",
      "[56496 rows x 2 columns]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "features =Predictors.columns\n",
    "train['log_loss'] =np.log(train['loss'])\n",
    "final_reg = RandomForestRegressor(n_estimators=500)\n",
    "for each in features:\n",
    "    train[each]=pd.factorize(train[each], sort=True)[0]\n",
    "    #test_set[each]=pd.factorize(test_set[each],sort=True)[0]\n",
    "#del test_set[\"id\"];\n",
    "train_array=np.array(train[features])\n",
    "mod=final_reg.fit(Predictors,train['log_loss'])\n",
    "test_array=np.array(test_set)\n",
    "final = np.exp(mod.predict(test_array))\n",
    "print(\"here\")\n",
    "submission = pd.read_csv('C:\\Users\\Vishnutej\\Desktop\\pml_example_submission.csv')\n",
    "submission.iloc[:, 1] = final\n",
    "submission.to_csv('C:\\Users\\Vishnutej\\Desktop\\\\file.csv', index=None)\n",
    "print(submission)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
